{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "#from ignite.engine import Engine, Events\n",
    "#from ignite.handlers import ModelCheckpoint, global_step_from_engine\n",
    "#from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n",
    "#from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n",
    "#from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n",
    "#from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer, GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n",
    "from pytorch_pretrained_bert import OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer\n",
    "from optim import AdamW\n",
    "#from utils import get_dataset, make_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n",
    "\n",
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    num_added_tokens = tokenizer.set_special_tokens(SPECIAL_TOKENS) # doesn't add if they are already there\n",
    "    model.set_num_special_tokens(len(SPECIAL_TOKENS))\n",
    "    #orig_num_tokens = len(tokenizer.encoder)\n",
    "    #num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there \n",
    "    #if num_added_tokens > 0:\n",
    "        #model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "\n",
    "#tokenizer_class = GPT2Tokenizer if \"gpt2\" in args.model_checkpoint else OpenAIGPTTokenizer\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "#model_class = GPT2DoubleHeadsModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
    "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
    "add_special_tokens_(model, tokenizer)\n",
    "#train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28,\n",
       " [-1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    \n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    \n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    else:\n",
    "        instance[\"lm_labels\"] = [-1] * len(instance[\"input_ids\"])\n",
    "        \n",
    "    return instance\n",
    "\n",
    "persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
    "           [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
    "history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "           [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "reply = [\"great\", \"to\", \"hear\"]\n",
    "I = build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False)\n",
    "len( I['input_ids']), I['lm_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_dataset(dataset, pd=0):\n",
    "    #for name in [\"input_ids\", \"lm_labels\", \"token_type_ids\"]:\n",
    "        #dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
    "    L = max(len(x) for x in dataset[\"input_ids\"])\n",
    "    dataset['input_ids'] = [ x + (L-len(x))*[pd] for x in dataset['input_ids'] ]\n",
    "    dataset['token_type_ids'] = [ x + (L-len(x))*[pd] for x in dataset['token_type_ids'] ]\n",
    "    dataset['lm_labels'] = [ x + (L-len(x))*[-1] for x in dataset['lm_labels'] ]\n",
    "    return dataset\n",
    "\n",
    "def pad_list(LIST, pad):\n",
    "    L = max(len(x) for x in LIST ) \n",
    "    return [ x + (L-len(x))*[pad] for x in LIST ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "NUM_CANDIDATES = 2 # Cap on number of Train Candidates\n",
    "PERSONALITY_PERM = 1 # Number of permutations of personality sentences\n",
    "MXHST_K = 2\n",
    "MAX_HISTORY = 2*(MXHST_K)+1 # Number of previous exchanges to keep in history\n",
    "\n",
    "def get_data_loaders(tokenizer):\n",
    "    personachat = torch.load('convai_data.tkn')\n",
    "\n",
    "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "    for dataset_name, dataset in personachat.items():\n",
    "        \n",
    "        #num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
    "        num_candidates =  20 #MAXIMUM NUM OF DISTRACTOR+GT_REPLY in our dataset\n",
    "        if dataset_name == 'train': num_candidates = min(NUM_CANDIDATES, num_candidates) # Number of candidates for training\n",
    "        datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
    "        \n",
    "        for dialog in dataset: #dialog= [ personality:[], utterances:[history:[], candidates:[]] ]\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            #for _ in range(PERSONALITY_PERM):  ----------------------------------------------\n",
    "            for utterance in dialog[\"utterances\"]:\n",
    "                history = utterance[\"history\"][-MAX_HISTORY:] #MAX_HISTORY per person\n",
    "                for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                    lm_labels = bool(j == num_candidates-1)\n",
    "                    instance = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
    "                    #for input_name, input_array in instance.items(): #datasets[dataset_name][input_name].append(input_array)\n",
    "                    datasets[dataset_name]['input_ids'].append( instance['input_ids'] )\n",
    "                    datasets[dataset_name]['token_type_ids'].append( instance['token_type_ids'] )\n",
    "                    datasets[dataset_name]['mc_token_ids'].append( instance['mc_token_ids'] )\n",
    "                    datasets[dataset_name]['lm_labels'].append( instance['lm_labels'] )\n",
    "                    \n",
    "                datasets[dataset_name][\"mc_labels\"].append(num_candidates-1) #TODO: make this 0\n",
    "                \n",
    "            #persona = [persona[-1]] + persona[:-1]  #permuted personalities\n",
    "            # PERSONALITY_PERM LOOP ----------------------------------------------------------\n",
    "    return personachat\n",
    "    personachat = None; del personachat\n",
    "    #dataset['train'/'valid'] = {'input_ids', 'lm_labels', 'token_type_ids', 'mc_token_ids'}\n",
    "    # The dataset contains lists that are grouped N=num_candidates objects\n",
    "    \n",
    "    #input_ids: sequence of token ids\n",
    "    #lm_labels: sequence of token ids with highlisted reply (lang modeling)\n",
    "    #token_type_ids: speaker annotation for each token\n",
    "    #mc_token_ids: length of input id-1 (some index that indicates when padding starts)\n",
    "    #mc_labels: index of the ground truth candidate (Multiple choice)\n",
    "    \n",
    "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        \n",
    "        #dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids('<pad>') ) ---------------------------------\n",
    "        pad = tokenizer.convert_tokens_to_ids('<pad>')\n",
    "        dataset['input_ids'] = pad_list( dataset['input_ids'], pad)\n",
    "        dataset['token_type_ids'] = pad_list( dataset['token_type_ids'], pad)\n",
    "        dataset['lm_labels'] = pad_list( dataset['lm_labels'], -1)\n",
    "        #-------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        for input_name in [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]:\n",
    "            tensor = torch.tensor(dataset[input_name])\n",
    "            if input_name != \"mc_labels\":\n",
    "                \n",
    "                #-----------------------------------------------------------------------------------------------------------\n",
    "                N,L = datasets[dataset_name][\"n_candidates\"], tensor.shape[1:]\n",
    "                tensor = tensor.view((-1, N) + L) \n",
    "                #L = tensor.shape[-1]; tensor = tensor.view((-1, N, L)) # Simpler version ----------------------------------\n",
    "                \n",
    "            tensor_datasets[dataset_name].append(tensor)\n",
    "\n",
    "    train_dataset = TensorDataset(*tensor_datasets[\"train\"])\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    #valid_dataset = TensorDataset(*tensor_datasets[\"valid\"])\n",
    "    #valid_loader = DataLoader(valid_dataset, batch_size=args.valid_batch_size, shuffle=False)\n",
    "    #train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None\n",
    "    #valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset) if args.distributed else None\n",
    "    return train_dataset#, valid_loader#, train_sampler, valid_sampler\n",
    "\n",
    "raw_dataset = get_data_loaders(tokenizer)\n",
    "torch.save(raw_dataset, 'raw_dataset.pyobj')\n",
    "#train_dataset = get_data_loaders(tokenizer)\n",
    "#torch.save(train_dataset, 'train_dataset.pyobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f623dd4fc88>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSPECT DATA\n",
    "\n",
    "train_dataset = torch.load('train_dataset.pyobj')\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "#\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"\n",
    "\n",
    "#input_ids: sequence of token ids\n",
    "#mc_token_ids: length of input id-1 (some index that indicates when padding starts)\n",
    "#lm_labels: sequence of token ids with highlisted reply (lang modeling)\n",
    "#mc_labels: index of the ground truth candidate (Multiple choice)\n",
    "#token_type_ids: speaker annotation for each token\n",
    "\n",
    "#print( D[0].shape, D[1].shape, D[2].shape, D[3].shape, D[4].shape )\n",
    "D = next( iter(train_loader) )\n",
    "#D = train_dataset[0]\n",
    "input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = D\n",
    "lm_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp\n",
    "LM_COEF = 1.0\n",
    "MC_COEF = 1.0\n",
    "DEVICE = 0\n",
    "FP16 = True\n",
    "MAX_NORM = 1.0 # Clipping Gradient Norm\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "train_batch_size = 4\n",
    "\n",
    "def update(b, batch):\n",
    "    # model.train()\n",
    "    \n",
    "    batch = [input_tensor.to(DEVICE) for input_tensor in batch]\n",
    "    input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "    \n",
    "    #(lm_loss), (mc_loss), *_ = model( input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids, mc_labels=mc_labels, lm_labels=lm_labels)\n",
    "    lm_loss, mc_loss = model(input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids) \n",
    "    \n",
    "    loss = (lm_loss * LM_COEF + mc_loss * MC_COEF) / GRAD_ACCUM_STEPS\n",
    "    \n",
    "    if FP16:\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), MAX_NORM)\n",
    "    else:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_NORM)\n",
    "        \n",
    "    if b % GRAD_ACCUM_STEPS == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "def inference(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = tuple(input_tensor.to(DEVICE) for input_tensor in batch)\n",
    "        input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "        logger.info(tokenizer.decode(input_ids[0, -1, :].tolist()))\n",
    "        # if we dont send labels to model, it doesnt return losses\n",
    "        lm_logits, mc_logits, *_ = model(\n",
    "            input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "        )\n",
    "        lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
    "        lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
    "        return (lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "keep_batchnorm_fp32    : None\n",
      "cast_model_type        : None\n",
      "opt_level              : O1\n",
      "loss_scale             : dynamic\n",
      "patch_torch_functions  : True\n",
      "enabled                : True\n",
      "master_weights         : None\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "keep_batchnorm_fp32    : None\n",
      "cast_model_type        : None\n",
      "opt_level              : O1\n",
      "loss_scale             : dynamic\n",
      "patch_torch_functions  : True\n",
      "enabled                : True\n",
      "master_weights         : None\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "0 0/32860 1.2796852588653564\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-efe6c18b9f9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8660427b4273>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(b, batch)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_NORM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/lib/python3.5/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/lib/python3.5/site-packages/apex/amp/wrap.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                      \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                      kwargs)\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;34mr\"\"\"See :func:`torch.norm`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/lib/python3.5/site-packages/apex/amp/wrap.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                      \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                      kwargs)\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/lib/python3.5/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#train_dataset = torch.load('train_dataset.pyobj')\n",
    "#train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "#train_loader = torch.load('dataloader.pyobj')\n",
    "model = model.cuda(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=6.25e-5, correct_bias=True)\n",
    "train_dataset = torch.load('train_dataset.pyobj')\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "if FP16: model, optimizer = amp.initialize(model, optimizer, opt_level='O1') #O1/O2 #https://nvidia.github.io/apex/amp.html\n",
    "\n",
    "EPOCHS = 100\n",
    "B = len(train_loader)\n",
    "for e in range(EPOCHS):\n",
    "    for b,batch in enumerate(train_loader):\n",
    "        loss = update( b, batch)\n",
    "        if b%(B//300) == 0: print(e,str(b)+'/'+str(B), loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor(10,2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.7370e+27,  4.5569e-41],\n",
       "        [ 1.6375e+22,  4.5569e-41],\n",
       "        [ 4.5113e+27,  4.5569e-41],\n",
       "        [ 4.6717e+27,  4.5569e-41],\n",
       "        [ 4.7370e+27,  4.5569e-41],\n",
       "        [-1.7097e+16,  4.5567e-41],\n",
       "        [ 4.5113e+27,  4.5569e-41],\n",
       "        [ 8.7380e+24,  4.5569e-41],\n",
       "        [ 4.7370e+27,  4.5569e-41],\n",
       "        [-1.3176e+17,  4.5567e-41]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "D = torch.load('dataloader.pyobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[40478,   249,   649,  ..., 40482, 40482, 40482],\n",
       "          [40478,   249,   649,  ..., 40482, 40482, 40482]],\n",
       " \n",
       "         [[40478,   249,   649,  ..., 40482, 40482, 40482],\n",
       "          [40478,   249,   649,  ..., 40482, 40482, 40482]]]),\n",
       " tensor([[100,  93],\n",
       "         [126, 123]]),\n",
       " tensor([[[-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100]],\n",
       " \n",
       "         [[-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100]]]),\n",
       " tensor([1, 1]),\n",
       " tensor([[[40480, 40480, 40480,  ..., 40482, 40482, 40482],\n",
       "          [40480, 40480, 40480,  ..., 40482, 40482, 40482]],\n",
       " \n",
       "         [[40480, 40480, 40480,  ..., 40482, 40482, 40482],\n",
       "          [40480, 40480, 40480,  ..., 40482, 40482, 40482]]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next( iter(D) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
