{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer\n",
    "from pytorch_pretrained_bert import OpenAIGPTLMHeadModel\n",
    "#from pytorch_pretrained_bert import OpenAIGPTDoubleHeadsModel\n",
    "\n",
    "from optim import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n",
    "\n",
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    num_added_tokens = tokenizer.set_special_tokens(SPECIAL_TOKENS) # doesn't add if they are already there\n",
    "    model.set_num_special_tokens(len(SPECIAL_TOKENS))\n",
    "    #orig_num_tokens = len(tokenizer.encoder)\n",
    "    #num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there \n",
    "    #if num_added_tokens > 0:\n",
    "        #model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "#model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "add_special_tokens_(model, tokenizer)\n",
    "weight = torch.load('/media/sec/conv_ai_weights/3.pth')\n",
    "model.load_state_dict( weight, strict= False)\n",
    "model.cuda(0)\n",
    "model.eval()\n",
    "\n",
    "dataset = torch.load('raw_dataset.pyobj')\n",
    "P = [dialog['personality'] for dialog in dataset['train']]\n",
    "\n",
    "print('hi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser.add_argument(\"--no_sample\", action='store_true', help=\"Set to use greedy decoding instead of sampling\")                                                                                                   \n",
    "NO_SAMPLE = False\n",
    "MAX_HISTORY = 2\n",
    "MAX_LENGTH = 20\n",
    "MIN_LENGTH = 1\n",
    "temperature = .7\n",
    "DEVICE = 0\n",
    "TOP_K= 0\n",
    "TOP_P= .9\n",
    "\n",
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    \n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    \n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    else:\n",
    "        instance[\"lm_labels\"] = [-1] * len(instance[\"input_ids\"])\n",
    "        \n",
    "    return instance\n",
    "\n",
    "\n",
    "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):    \n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate    \\ a bit the code                                                                                    \n",
    "    top_k = min(top_k, logits.size(-1))                                                                 \n",
    "    if top_k > 0:                                                                                       \n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens             \n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]                        \n",
    "        logits[indices_to_remove] = filter_value                                                        \n",
    "                                                                                                        \n",
    "    if top_p > 0.0:                                                                                     \n",
    "        # Compute cumulative probabilities of sorted tokens                                             \n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)                             \n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)               \n",
    "                                                                                                        \n",
    "        # Remove tokens with cumulative probability above the threshold                                 \n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p                                     \n",
    "        # Shift the indices to the right to keep also the first token above the threshold               \n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()                  \n",
    "        sorted_indices_to_remove[..., 0] = 0                                                            \n",
    "                                                                                                        \n",
    "        # Back to unsorted indices and set them to -infinity                                            \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]                                    \n",
    "        logits[indices_to_remove] = filter_value                                                        \n",
    "                                                                                                        \n",
    "    indices_to_remove = logits < threshold                                                              \n",
    "    logits[indices_to_remove] = filter_value                                                            \n",
    "                                                                                                        \n",
    "    return logits  \n",
    "\n",
    "def sample_sequence(personality, history, tokenizer, model, current_output=None):                 \n",
    "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)                                \n",
    "    if current_output is None:                                                                          \n",
    "        current_output = []                                                                             \n",
    "                                                                                                        \n",
    "    for i in range(MAX_LENGTH):                                                                    \n",
    "        instance = build_input_from_segments(personality, history, current_output, tokenizer, with_eos=False)                                                                                          \n",
    "                                                                                                        \n",
    "        input_ids = torch.tensor(instance[\"input_ids\"], device=DEVICE).unsqueeze(0)                \n",
    "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=DEVICE).unsqueeze(0)      \n",
    "                                                                                                        \n",
    "        logits = model(input_ids, token_type_ids=token_type_ids)                                        \n",
    "        if isinstance(logits, tuple):  # for gpt2 and maybe others                                      \n",
    "            logits = logits[0]                                                                          \n",
    "        logits = logits[0, -1, :] / temperature                                                    \n",
    "        logits = top_filtering(logits, top_k=TOP_K, top_p=TOP_P)                              \n",
    "        probs = F.softmax(logits, dim=-1)                                                               \n",
    "                                                                                                        \n",
    "        prev = torch.topk(probs, 1)[1] if NO_SAMPLE else torch.multinomial(probs, 1)               \n",
    "        if i < MIN_LENGTH and prev.item() in special_tokens_ids:                                   \n",
    "            while prev.item() in special_tokens_ids:                                                    \n",
    "                if probs.max().item() == 1:                                                             \n",
    "                    warnings.warn(\"Warning: model generating special token with probability 1.\")        \n",
    "                    break  # avoid infinitely looping over special token                                \n",
    "                prev = torch.multinomial(probs, num_samples=1)                                          \n",
    "                                                                                                        \n",
    "        if prev.item() in special_tokens_ids:                                                           \n",
    "            break                                                                                       \n",
    "        current_output.append(prev.item())                                                              \n",
    "                                                                                                        \n",
    "    return current_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i work as a contractor for a cab company. i'm engaged. i love rock music. i'm taking courses online. i like ice cream.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello! how are you doing?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  I am doing well\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cool. where are you from?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  I am from rock school\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh wow. i am a contractor for a cab company\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  that is great!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what kind of food do you like? ice cream is my favorite food!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  I love tacos do you like tacos?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like ice cream\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  ah okay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you like to listen to rock music?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  yes I love\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am a rock singer in my spare time\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  That is awesome\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what do you do for a living\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  I like to sing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love rock music\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  Me too!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am a contractor for a cab company\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  Ok thanks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that's cool\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  good bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm going to see my band play in a little bit\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch.nn.functional as F \n",
    "\n",
    "p = random.choice(P)\n",
    "print( tokenizer.decode( chain(*p) ) )\n",
    "\n",
    "history = []\n",
    "while True:\n",
    "    raw_text = input('>>> ')\n",
    "    while not raw_text:                                                                      \n",
    "        print('Prompt should not be empty!')                                                        \n",
    "        raw_text = input(\">>> \")                                                                    \n",
    "    history.append(tokenizer.encode(raw_text))                                                      \n",
    "    with torch.no_grad():                                                                           \n",
    "        out_ids = sample_sequence(p, history, tokenizer, model)                     \n",
    "    history.append(out_ids)                                                                         \n",
    "    history = history[-(2*MAX_HISTORY+1):]                                                     \n",
    "    out_text = tokenizer.decode(out_ids, skip_special_tokens=True)                                  \n",
    "    print(out_text)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
